{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9e9393e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client, wrappers\n",
    "from openevals.llm import create_llm_as_judge\n",
    "from openevals.prompts import CORRECTNESS_PROMPT\n",
    "from app.services.llm_service import HFChatModel\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "74f820c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['0a7b02cc-138a-41a3-bbb9-279396c20ff2',\n",
       "  'b02d6815-e98e-4c7f-abfa-0f02b79efdbb',\n",
       "  '40ec0d2a-9773-464e-8524-86c2c4a2d677'],\n",
       " 'count': 3}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the input and reference output pairs that you'll use to evaluate your app\n",
    "client = Client()\n",
    "\n",
    "# Create the dataset\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=\"Vid Query dataset\", description=\"A Vid Query dataset in LangSmith.\"\n",
    ")\n",
    "\n",
    "# Create examples in the dataset. Examples consist of inputs and reference outputs \n",
    "examples = [\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is Nihilism?\"},\n",
    "        \"outputs\": {\"answer\": \"Nihilism is a philosophical belief that denies the existence of any objective meaning, purpose, or intrinsic value in life. It asserts that all values are human-created constructs and that there is no inherent good or evil in the world. Nihilism can take various forms, such as political nihilism, which advocates for the destruction of all political, social, and religious order, or ethical nihilism, which rejects the idea of absolute ethical or moral values.\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What's the difference between cynicism and nihilism?\"},\n",
    "        \"outputs\": {\"answer\": \"Cynicism and nihilism are distinct philosophical perspectives, although they share some similarities. Cynicism posits that people are primarily motivated by self-interest and do not have intrinsically good motives, while nihilism denies the existence of any inherent meaning, value, or purpose in the universe or human life. While cynics may be pessimistic about human nature, they do not reject the existence of good and evil, unlike nihilists who view these concepts as human constructs with no objective reality.\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What did Friedrich Nietzsche say about Nihilism?\"},\n",
    "        \"outputs\": {\"answer\": \"Friedrich Nietzsche argued for nihilism in the sense that he believed there is no objective structure or order in the world except the one we create for ourselves. He also stated that every belief, every considering something true is necessarily false, because there is simply no true world. However, he also expressed concerns about nihilism, stating that in the coming centuries, the advent of nihilism would drive civilization towards a catastrophe, a disaster waiting to implode.\"},\n",
    "    }\n",
    "]\n",
    "\n",
    "# Add the examples to the dataset\n",
    "client.create_examples(dataset_id=dataset.id, examples=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12acc02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.services.transcript_fetcher import fetch_youtube_transcript\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "da041ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using transcript in preferred language: en\n"
     ]
    }
   ],
   "source": [
    "transcript = fetch_youtube_transcript(video_id=\"ZOvyn72x6kQ\")\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "253cb2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from app.services.embedding_service import HFInferenceEmbeddings\n",
    "import os\n",
    "\n",
    "chunks = splitter.split_text(transcript)\n",
    "vectorstore = FAISS.from_texts(\n",
    "    texts=chunks,\n",
    "    embedding=HFInferenceEmbeddings(\n",
    "    model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    token=os.getenv(\"HF_TOKEN\")\n",
    ")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8238d69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='fd952deb-aa24-4c0d-bb54-d926aa4f2b3b', metadata={}, page_content='becomes something that you believe in. But since you now believe in something, then\\nthere is no nihilism, because nihilism is the belief that there is nothing. Nihilism is quite different from other philosophical\\nideas because it was first a literary invention before it ever became philosophical. As a result, it’s not as clearly defined\\nas many of the other philosophies that exist. Many different people explained it in different'),\n",
       " Document(id='496dbc48-ab05-4ef7-8086-24d8136e448c', metadata={}, page_content='as many of the other philosophies that exist. Many different people explained it in different\\nways, but eventually these different definitions got categorized, forming many different kinds\\nof nihilism. There’s political nihilism. Political nihilists believe that for humanity\\nto move forward as a species, all political, social, and religious order must be destroyed. Then there’s ethical nihilism. It rejects the idea of absolute ethical or\\nmoral values. With this type of nihilism, good or bad is'),\n",
       " Document(id='a09d9df4-5620-4c26-bb11-aa289593e554', metadata={}, page_content='that they come to the conclusion that there is no why. There is no answer, there is simply nothing. As Alan Watts once wrote, “life is nothing\\nmore than a trip from the maternity ward to the crematorium.” It’s really in the name, the term “nihilism”\\ncomes from the Latin word “nihil” which translates to “nothing,” and “ism”\\nwhich translates to ideology. It’s the ideology of nothing, but that doesn’t\\nreally help us in understanding it completely. Usually, people confuse nihilism for pessimism,')]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search(query=\"What is Nihilism?\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "25f1fb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HFChatModel(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    provider=\"together\",\n",
    "    token=os.environ[\"HF_TOKEN\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Define your RAG prompt\n",
    "rag_prompt = PromptTemplate.from_template(\"\"\"\n",
    "- You are a helpful assistant who is good at analyzing source information and answering questions.\n",
    "- Use the following source documents to answer the user's questions.\n",
    "- If you don't know the answer, just say that you don't know.\n",
    "- Use three sentences maximum and keep the answer concise.\n",
    "- Do not make up answers or provide information that is not in the source documents.\n",
    "- If the question is not related to the source documents, say \"No information available in the source documents.\"\n",
    "\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer in a professional tone.\n",
    "\"\"\")\n",
    "\n",
    "# Chain that combines prompt + LLM\n",
    "# qa_chain = LLMChain(llm=llm, prompt=rag_prompt)\n",
    "qa_chain = rag_prompt | llm\n",
    "\n",
    "def get_rag_response(question: str, vectorstore: FAISS = vectorstore, k: int = 3) -> str:\n",
    "    \n",
    "    if not vectorstore:\n",
    "        # result = qa_chain.invoke({\"context\": \"There's no video content available but answer any generic questions.\", \"question\": question})   # Can later allow general chats with LLM\n",
    "        return \"No video content available. Please analyze a video first.\"\n",
    "    \n",
    "    # Retrieve top k relevant chunks\n",
    "    docs = vectorstore.similarity_search(question, k=k)\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "\n",
    "    # Ask LLM with context\n",
    "    result = qa_chain.invoke(\n",
    "        {\"context\": context, \"question\": question}\n",
    "    )\n",
    "    return {\"answer\": result.content.strip(), \"documents\": context}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1d2f5c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nihilism is a philosophical belief that denies the existence of any objective meaning, purpose, or intrinsic value in life or the universe. It asserts that nothing has any inherent significance or necessity, and that human-created concepts such as morality, good, and evil are subjective and arbitrary.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_rag_response(\"What is Nihilism?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3341dd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"An evaluator for RAG answer accuracy\"\"\"\n",
    "    instructions = \"\"\"You are a teacher grading a quiz. \n",
    "\n",
    "You will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "(1) Grade the student answers based ONLY on their factual accuracy relative to the ground truth answer. \n",
    "(2) Ensure that the student answer does not contain any conflicting statements.\n",
    "(3) It is OK if the student answer contains more information than the ground truth answer, as long as it is factually accurate relative to the  ground truth answer.\n",
    "\n",
    "Correctness:\n",
    "A correctness value of True means that the student's answer meets all of the criteria.\n",
    "A correctness value of False means that the student's answer does not meet all of the criteria.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "    answers = f\"\"\"\\\n",
    "QUESTION: {inputs['question']}\n",
    "GROUND TRUTH ANSWER: {reference_outputs['answer']}\n",
    "STUDENT ANSWER: {outputs['answer']}\"\"\"\n",
    "\n",
    "    # Add explicit instruction for JSON output\n",
    "    answers = (\n",
    "        answers\n",
    "        + \"\\n\\nRespond ONLY in the following JSON format:\\n\"\n",
    "        + '{ \"explanation\": \"...\", \"correct\": true/false }'\n",
    "    )\n",
    "\n",
    "    # Run evaluator\n",
    "    response = llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": answers}\n",
    "    ])\n",
    "\n",
    "    # Parse JSON from model output\n",
    "    try:\n",
    "        # If response is an object with .content, extract it\n",
    "        result = json.loads(response.content.strip())\n",
    "        return result[\"correct\"]\n",
    "    except Exception as e:\n",
    "        print(\"Failed to parse grader output:\", e)\n",
    "        print(\"Raw output:\", response)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0ce57f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"An evaluator for RAG answer accuracy\"\"\"\n",
    "    instructions = \"\"\"You are a teacher grading a quiz. \n",
    "\n",
    "You will be given a QUESTION and a STUDENT ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "(1) Ensure the STUDENT ANSWER is concise and relevant to the QUESTION\n",
    "(2) Ensure the STUDENT ANSWER helps to answer the QUESTION\n",
    "\n",
    "Relevance:\n",
    "A relevance value of True means that the student's answer meets all of the criteria.\n",
    "A relevance value of False means that the student's answer does not meet all of the criteria.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "    answers = f\"\"\"\\\n",
    "QUESTION: {inputs['question']}\n",
    "GROUND TRUTH ANSWER: {reference_outputs['answer']}\n",
    "STUDENT ANSWER: {outputs['answer']}\"\"\"\n",
    "\n",
    "    # Add explicit instruction for JSON output\n",
    "    answers = (\n",
    "        answers\n",
    "        + \"\\n\\nRespond ONLY in the following JSON format:\\n\"\n",
    "        + '{ \"explanation\": \"...\", \"relevant\": true/false }'\n",
    "    )\n",
    "\n",
    "    # Run evaluator\n",
    "    response = llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": answers}\n",
    "    ])\n",
    "    \n",
    "    # Parse JSON from model output\n",
    "    try:\n",
    "        # If response is an object with .content, extract it\n",
    "        result = json.loads(response.content.strip())\n",
    "        return result[\"relevant\"]\n",
    "    except Exception as e:\n",
    "        print(\"Failed to parse grader output:\", e)\n",
    "        print(\"Raw output:\", response)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "533d2d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def groundedness(inputs: dict, outputs: dict) -> bool:\n",
    "    \"\"\"An evaluator for RAG answer accuracy\"\"\"\n",
    "    instructions = \"\"\"You are a teacher grading a quiz. \n",
    "\n",
    "You will be given FACTS and a STUDENT ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "(1) Ensure the STUDENT ANSWER is grounded in the FACTS. \n",
    "(2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.\n",
    "\n",
    "Grounded:\n",
    "A grounded value of True means that the student's answer meets all of the criteria.\n",
    "A grounded value of False means that the student's answer does not meet all of the criteria.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "    answer = f\"FACTS: {outputs[\"documents\"]}\\nQUESTION: {inputs['question']}\"\n",
    "\n",
    "    # Add explicit instruction for JSON output\n",
    "    answer = (\n",
    "        answer\n",
    "        + \"\\n\\nRespond ONLY in the following JSON format:\\n\"\n",
    "        + '{ \"explanation\": \"...\", \"grounded\": true/false }'\n",
    "    )\n",
    "\n",
    "    # Run evaluator\n",
    "    response = llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": answer}\n",
    "    ])\n",
    "    \n",
    "    # Parse JSON from model output\n",
    "    try:\n",
    "        # If response is an object with .content, extract it\n",
    "        result = json.loads(response.content.strip())\n",
    "        return result[\"grounded\"]\n",
    "    except Exception as e:\n",
    "        print(\"Failed to parse grader output:\", e)\n",
    "        print(\"Raw output:\", response)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3535fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_relevance(inputs: dict, outputs: dict) -> bool:\n",
    "    \"\"\"An evaluator for document relevance\"\"\"\n",
    "    instructions = \"\"\"You are a teacher grading a quiz. \n",
    "\n",
    "You will be given a QUESTION and a set of FACTS provided by the student. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "(1) You goal is to identify FACTS that are completely unrelated to the QUESTION\n",
    "(2) If the facts contain ANY keywords or semantic meaning related to the question, consider them relevant\n",
    "(3) It is OK if the facts have SOME information that is unrelated to the question as long as (2) is met\n",
    "\n",
    "Relevance:\n",
    "A relevance value of True means that the FACTS contain ANY keywords or semantic meaning related to the QUESTION and are therefore relevant.\n",
    "A relevance value of False means that the FACTS are completely unrelated to the QUESTION.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "    answer = f\"FACTS: {outputs[\"documents\"]}\\nQUESTION: {inputs['question']}\"\n",
    "\n",
    "    # Add explicit instruction for JSON output\n",
    "    answer = (\n",
    "        answer\n",
    "        + \"\\n\\nRespond ONLY in the following JSON format:\\n\"\n",
    "        + '{ \"explanation\": \"...\", \"relevant\": true/false }'\n",
    "    )\n",
    "\n",
    "    # Run evaluator\n",
    "    response = llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": answer}\n",
    "    ])\n",
    "    \n",
    "    # Parse JSON from model output\n",
    "    try:\n",
    "        # If response is an object with .content, extract it\n",
    "        result = json.loads(response.content.strip())\n",
    "        return result[\"relevant\"]\n",
    "    except Exception as e:\n",
    "        print(\"Failed to parse grader output:\", e)\n",
    "        print(\"Raw output:\", response)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cfbb8642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-doc-relevance-2df622df' at:\n",
      "https://smith.langchain.com/o/016d1287-5053-4244-8348-7d1be6ad628a/datasets/a9aa81a5-2dc4-486c-b92f-bc54f07fb16e/compare?selectedSessions=99b9fecb-116e-4296-b437-9f72ceb72a72\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:05,  5.90s/it]Error running evaluator <DynamicRunEvaluator relevance> on run 7e1f6fbb-7e26-472a-a13a-93fb3af37736: HfHubHTTPError('402 Client Error: Payment Required for url: https://router.huggingface.co/together/v1/chat/completions (Request ID: Root=1-6881f5e6-236ffb1b714cb15959f753c8;3f5b98c7-eaba-4af1-9053-1d88713f3cba)\\n\\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py\", line 409, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\requests\\models.py\", line 1026, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 402 Client Error: Payment Required for url: https://router.huggingface.co/together/v1/chat/completions\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1603, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 692, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Sam\\AppData\\Local\\Temp\\ipykernel_59280\\1313421810.py\", line 32, in relevance\n",
      "    response = llm.invoke([\n",
      "               ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 980, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 799, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1049, in _generate_with_cache\n",
      "    result = self._generate(messages, stop=stop, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\app\\services\\llm_service.py\", line 27, in _generate\n",
      "    result = self._client.chat.completions.create(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py\", line 924, in chat_completion\n",
      "    data = self._inner_post(request_parameters, stream=stream)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py\", line 280, in _inner_post\n",
      "    hf_raise_for_status(response)\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py\", line 482, in hf_raise_for_status\n",
      "    raise _format(HfHubHTTPError, str(e), response) from e\n",
      "huggingface_hub.errors.HfHubHTTPError: 402 Client Error: Payment Required for url: https://router.huggingface.co/together/v1/chat/completions (Request ID: Root=1-6881f5e6-236ffb1b714cb15959f753c8;3f5b98c7-eaba-4af1-9053-1d88713f3cba)\n",
      "\n",
      "You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\n",
      "Error running evaluator <DynamicRunEvaluator retrieval_relevance> on run 7e1f6fbb-7e26-472a-a13a-93fb3af37736: HfHubHTTPError('402 Client Error: Payment Required for url: https://router.huggingface.co/together/v1/chat/completions (Request ID: Root=1-6881f5e6-64b533e63c37a175727e0fbd;28c9c768-887c-4f18-9f14-2f803f352223)\\n\\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py\", line 409, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\requests\\models.py\", line 1026, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 402 Client Error: Payment Required for url: https://router.huggingface.co/together/v1/chat/completions\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1603, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 692, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Sam\\AppData\\Local\\Temp\\ipykernel_59280\\2295391256.py\", line 30, in retrieval_relevance\n",
      "    response = llm.invoke([\n",
      "               ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 980, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 799, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1049, in _generate_with_cache\n",
      "    result = self._generate(messages, stop=stop, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\app\\services\\llm_service.py\", line 27, in _generate\n",
      "    result = self._client.chat.completions.create(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py\", line 924, in chat_completion\n",
      "    data = self._inner_post(request_parameters, stream=stream)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py\", line 280, in _inner_post\n",
      "    hf_raise_for_status(response)\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py\", line 482, in hf_raise_for_status\n",
      "    raise _format(HfHubHTTPError, str(e), response) from e\n",
      "huggingface_hub.errors.HfHubHTTPError: 402 Client Error: Payment Required for url: https://router.huggingface.co/together/v1/chat/completions (Request ID: Root=1-6881f5e6-64b533e63c37a175727e0fbd;28c9c768-887c-4f18-9f14-2f803f352223)\n",
      "\n",
      "You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\n",
      "2it [00:10,  4.90s/it]Error running target function: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/sentence-transformers/all-MiniLM-L6-v2/pipeline/feature-extraction (Request ID: Root=1-6881f5e6-300d393c13a7cb1548b47254;dcbfd100-3f4c-4380-8e9c-ef3245181acf)\n",
      "\n",
      "You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py\", line 409, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\requests\\models.py\", line 1026, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/sentence-transformers/all-MiniLM-L6-v2/pipeline/feature-extraction\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1907, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 692, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Sam\\AppData\\Local\\Temp\\ipykernel_59280\\137039935.py\", line 2, in target\n",
      "    return get_rag_response(inputs[\"question\"])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Sam\\AppData\\Local\\Temp\\ipykernel_59280\\169999051.py\", line 38, in get_rag_response\n",
      "    docs = vectorstore.similarity_search(question, k=k)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py\", line 643, in similarity_search\n",
      "    docs_and_scores = self.similarity_search_with_score(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py\", line 515, in similarity_search_with_score\n",
      "    embedding = self._embed_query(query)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py\", line 266, in _embed_query\n",
      "    return self.embedding_function.embed_query(text)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\app\\services\\embedding_service.py\", line 12, in embed_query\n",
      "    return self.client.feature_extraction(text)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py\", line 1087, in feature_extraction\n",
      "    response = self._inner_post(request_parameters)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py\", line 280, in _inner_post\n",
      "    hf_raise_for_status(response)\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py\", line 482, in hf_raise_for_status\n",
      "    raise _format(HfHubHTTPError, str(e), response) from e\n",
      "huggingface_hub.errors.HfHubHTTPError: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/sentence-transformers/all-MiniLM-L6-v2/pipeline/feature-extraction (Request ID: Root=1-6881f5e6-300d393c13a7cb1548b47254;dcbfd100-3f4c-4380-8e9c-ef3245181acf)\n",
      "\n",
      "You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\n",
      "Error running evaluator <DynamicRunEvaluator correctness> on run c27795c0-8155-4dc5-9758-51621b97e720: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1603, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 692, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Sam\\AppData\\Local\\Temp\\ipykernel_59280\\3127047322.py\", line 25, in correctness\n",
      "    STUDENT ANSWER: {outputs['answer']}\"\"\"\n",
      "                     ~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator groundedness> on run c27795c0-8155-4dc5-9758-51621b97e720: KeyError('documents')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1603, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 692, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Sam\\AppData\\Local\\Temp\\ipykernel_59280\\2957737590.py\", line 19, in groundedness\n",
      "    answer = f\"FACTS: {outputs[\"documents\"]}\\nQUESTION: {inputs['question']}\"\n",
      "                       ~~~~~~~^^^^^^^^^^^^^\n",
      "KeyError: 'documents'\n",
      "Error running evaluator <DynamicRunEvaluator relevance> on run c27795c0-8155-4dc5-9758-51621b97e720: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1603, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 692, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Sam\\AppData\\Local\\Temp\\ipykernel_59280\\1313421810.py\", line 22, in relevance\n",
      "    STUDENT ANSWER: {outputs['answer']}\"\"\"\n",
      "                     ~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator retrieval_relevance> on run c27795c0-8155-4dc5-9758-51621b97e720: KeyError('documents')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1603, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 692, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sam\\Desktop\\Test\\vid-query\\backend\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Sam\\AppData\\Local\\Temp\\ipykernel_59280\\2295391256.py\", line 20, in retrieval_relevance\n",
      "    answer = f\"FACTS: {outputs[\"documents\"]}\\nQUESTION: {inputs['question']}\"\n",
      "                       ~~~~~~~^^^^^^^^^^^^^\n",
      "KeyError: 'documents'\n",
      "3it [00:10,  3.65s/it]\n"
     ]
    }
   ],
   "source": [
    "def target(inputs: dict) -> dict:\n",
    "    return get_rag_response(inputs[\"question\"])\n",
    "\n",
    "experiment_results = client.evaluate(\n",
    "    target,\n",
    "    data='Vid Query dataset',\n",
    "    evaluators=[correctness, groundedness, relevance, retrieval_relevance],\n",
    "    experiment_prefix=\"rag-doc-relevance\",\n",
    "    metadata={\"model\": \"Mistral-7B-Instruct-v0.3\"},\n",
    ")\n",
    "# Explore results locally as a dataframe if you have pandas installed\n",
    "# experiment_results.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
